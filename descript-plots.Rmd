# Descriptive Plots {#descplot}

This chapter  presents several descriptive plots of the raw data. No statistical analyses, no standard errors, no parameters to estimate, no hypotheses to test. Just raw data presented in different forms to gain insight into the patterns that are in the data.  I'll create a few plots at the world level, and then switch to the state and county levels for US data.  You are free to take my code and edit it to produce additional plots for other countries and their states or provinces.  Throughout these notes I'll focus on the counts and rates of  positive tests. I'll defer death counts until the chapter on process models. 

As stated in a few places in these notes, we need to be careful in how we interpret the counts of people who test positive.  There are many issues such as countries or states that test more frequently may see higher counts because they do  more testing, counts can vary across countries or states because decisions about who is tested may vary, the quality of the tests used may vary across units, and countless other differences that make it extremely difficult to generate clear explanations for patterns we may see.   Moving to other variables such as the number of deaths doesn't completely solve the problem either because, among other things, countries and states may vary in how they count deaths (e.g., death in a hospital, death among people who tested positive vs. deaths that are ["presumed to be covid-19 related"](https://www.washingtonpost.com/investigations/which-deaths-count-toward-the-covid-19-death-toll-it-depends-on-the-state/2020/04/16/bca84ae0-7991-11ea-a130-df573469f094_story.html)).

## World Map


Some code I copied from [here](https://datascienceplus.com/map-visualization-of-covid19-across-world/).

I make use of the datacov.World file I read in from the Johns Hopkins git repository.



```{r}
# for these code chunks I use the datacov.World data file
# coding trick; by switching the definition of datacov I can keep my original code post the 3/23/20 change and just swap 
# out which file, world or us, to drop into datacov.  Rest of code should remain the same as prior to the change.
datacov <- datacov.World

# cutoffs and labels to use in the plot based on the number of cases
mybreaks <- c(1, 20, 100, 1000, 50000)
mylabels <- c("1-19", "20-99", "100-999", "1,000-49,999", "50,000+")

world <- map_data("world")

ggplot() +
 geom_polygon(data = world, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
 geom_point(data=datacov, aes(x=Long, y=Lat, size=datacov[[ncol(datacov)]], color=datacov[[ncol(datacov)]]),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) + 
  labs(caption = 
  "Data Repository by Johns Hopkins CSSE. Visualization by DataScience+ ") +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)
    )

```

You'll notice that some countries like Canada, UK, Australia have multiple points but most countries are represented by only 1 point. This is because the data set we are using has some inconsistencies in reporting either country totals or  state/province totals. This would require further cleaning (e.g., computing country totals for those countries) but I left this in here as a teaching moment of how one needs to be careful working with data sets and consistently double check your assumptions about the structure of the data set you are using. The structure may also change over time so one needs to monitor carefully.

Even more sophisticated is the  [interactive map](https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6) developed at Johns Hopkins (same group that provides the key data we downloaded in Chapter \@ref(readintro)), where you can zoom the map in and out, click on different countries (left panel) and see data for that country on the right panel. A small interactive display of that website is displayed below.

```{r echo=F}
knitr::include_url("https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6", height="600px")
```

### World Map with Time Animation {#worldanim}
\index{animation}

I'll expand this example with some of my code to create an animation so we can see the total cases in the world map change by day.  

```{asis echo=longform}
We need to reformat the data file datacov into long format in order to be used by the animation function in the package gganimate.  That is, rather than using the datacov data.frame that is in wide format, I need to reformat the data.frame into long format and create a day column and a count column.  The gather() function in dplyr does this very efficiently.  To help in later analyses I also create a day.numeric variable that is 0 on 3/08/20 and increments by one each day after.  I'll explain later why I set 3/08/20 as day 0.
```

```{r echo=longform}
datacov.long <- gather(datacov, day, count, `1/22/20`:(names(datacov)[ncol(datacov)]))
datacov.long$day <- as.Date(datacov.long$day, "%m/%d/%y")
datacov.long$day.numeric <- as.numeric(datacov.long$day)-18283
```

```{asis echo=longform}
I rewrote the original parts of the plotting program to make it more efficient for the animation (e.g., I use the ggthemes in the maps package). This animation uses the package gganimate to reproduce the plot for each day and then overlays the plots one on top of the other to produce the animation.
```

<!-- trying two different versions for html and latex -->

```{r cache=cache,fig.cap="World Count of Positive Cases", eval=knitr::is_html_output(), echo=longform&knitr::is_html_output()}

ggplot(datacov.long) +  borders("world", colour = "gray90", fill = "gray85") +
  theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=count, color=count),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

```{r cache=cache, fig.cap="World Count of Positive Cases", fig.show="animate", aniopts=c("autoplay", "controls", "loop"),  eval=knitr::is_latex_output(),echo=longform&knitr::is_latex_output()}
ggplot(datacov.long) +  borders("world", colour = "gray90", fill = "gray85") +
  theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=count, color=count),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

While counts are useful (see the [Introduction](#intro) for a discussion of London's cholera epidemic) they have their limitations. The help address issues with different countries having different populations, it is common to normalize counts by the population and report numbers per capita (e.g., 62 cases out of 100,000).  One would have to track down the populations of each country, download the relevant population data using methods described in Chapter \@ref(readintro) and merge that information into this animation.  Later in this chapter, I develop the analogous animation for the US and I already downloaded the state populations so will illustrate the difference in this animation in counts versus per capita   However, raw counts still have a role in helping to inform the impact of covid-19.  If a city has 1500 available hospital beds (and presumably the staff and supplies to provide care for  those  beds), but there are 2000 people in need of hospitalization, then there is a public health issue and the 'per capita" concern becomes moot.

For more information on working with maps see [Gimond](https://mgimond.github.io/Spatial/index.html).

## Compare World Counts to US

It would be helpful to see the pattern over time of the world counts of confirmed covid-19 cases relative to the US counts.  Here I create a subset of the datacov data.frame that includes just the 50 states (I dropped DC and the US territories for this analysis).

```{r}
datacov.temp <- datacov.US
allstates.temp <- subset(datacov.temp, `Province/State`%in% states)
allstates.temp2 <- allstates.temp %>% group_by(`Province/State`) %>%  summarize_at(vars(`1/22/20`:(names(allstates.temp)[ncol(allstates.temp)])), sum,na.rm=T)
# need state level long and lat (these are county) so just replace these sums with the ones in the old allstates

# create subset of 50 states from original datacov in order to have the correct long and lat by state (rather than county)
# this is a check that new code from 3/31/20 works with the older format
datacov.ori <- 
 read_csv("/Users/gonzo/Dropbox/transfer/mac transfer/covid19-analyses/time_series_19-covid-Confirmed.csv")
allstates <- subset(datacov.ori, `Province/State`%in% states)
allstates.ori <- allstates

allstates.temp2 <- allstates.temp2[order(match(allstates.temp2$`Province/State`,allstates$`Province/State`)),]
# double check ordering of rows
#head(cbind(allstates$`Province/State`, allstates.temp2$`Province/State`))

# use state level long and lat from datacov.ori
allstates.temp2$Lat <- allstates$Lat
allstates.temp2$Long <- allstates$Long
allstates.temp2$`Country/Region` <- allstates.ori$`Country/Region`

# reorder columns to be like allstates.ori so subsequent code works
allstates <- allstates.temp2[,c(1,ncol(allstates.temp2), ncol(allstates.temp2)-2, ncol(allstates.temp2)-1, 2:(ncol(allstates.temp2)-3))]

# now everything is set so new data structure looks like the old data structure

# prettyval is an extra column appended at the end; it has population size and state abbreviation like "19.5 NY"
# need to be sure the row orders of state.population that we downloaded from the census website
# match the order of states in the allstates subset of the datacov data.frame
allstates$prettyval <- state.population$prettyval[order(match(state.population$NAME,allstates$`Province/State`))]

# likewise, save state populations from the census file to the allstates data.frame
allstates$population <- state.population$value[order(match(state.population$NAME,allstates$`Province/State`))]

# add poverty data
allstates$povertyperc <- statepoverty$percent[order(match(statepoverty$state,allstates$`Province/State`))]
     
# number of variables added (prettyval and population) to the allstates data.frame
cadded <- 3
# counts by dates are in columns starting with `1/22/20` and ending in ncol(allstates)-3 because
# we appended two columns at the end
allstates.long <- gather(allstates, day, count, 
                         `1/22/20`:(names(allstates)[ncol(allstates)-cadded]))

# reformat the day column to be Dates that R can understand
allstates.long$day <- as.Date(allstates.long$day, "%m/%d/%y")

# in some analyses it may be helpful to look at first order differences in counts (day (t+1) - day (t))
# so far I don't need this but I leave it here
#allstates.long$count.diff <- c(0,diff(allstates.long$count))
```

```{asis echo=longform}
More data manipulation needed.  Turns out that the Johns Hopkins data did not report state-level data prior to 3/08/2020 (just US totals apparently).  So, for state-level analyses it may be helpful I only use dates after 03/08/2020.  Also, in the comments I explain how R handles dates with a reference date of 01/01/1970. 
```

```{r}
allstates.long <- subset(allstates.long, day > "2020-03-08")

# in case we need the last dates for each state
# data_set_ends <- allstates.long %>% group_by(`Province/State`) %>% top_n(1,day) %>% pull(prettyval)

# create day.numeric starting with 3/09/20 as day 1.  in R, Dates have
# 01/01/1970 and 03/08/20 is 18330 days from the origin so need to subtract
# 18330 from all the days. Essentially, I'm centering at 03/08/20
allstates.long$day.numeric <- as.numeric(allstates.long$day)-18330

# make the variable with the state abbreviations a factor in R
allstates.long$state.abb <- factor(allstates.long$`Province/State`)

# create per capita (out of 100)
allstates.long$percap100 <- allstates.long$count/allstates$population * 100
```
```{asis echo=longform}
Now that we have allstates.long created we can compare cumulative counts in the World to the US cumulative counts.  For a definition of the multiplot() function see the [Preliminaries chapter](#multiplot).
```

```{r fig.cap="Cumulative Counts of Positive Cases"}
# compute sums for the entire world by day and store plot in object p1
sum.world <- data.frame(count=apply(datacov.World[,5:ncol(datacov.World)],2,sum,na.rm=T))
sum.world$day <- as.Date(rownames(sum.world), "%m/%d/%y")
p1 <- ggplot(sum.world, aes(x=day,y=count))  + geom_bar(stat="identity",fill="steel blue") + ggtitle("World Wide Counts") + theme_minimal() + scale_y_continuous(labels=comma)
# above line: labels=comma prints y axis labels as numbers rather than scientific notation
# can print plot p1 by itself by uncommenting next line
#p1

# using weight option rather than y so that bar stat_count can do the sum (saves having to compute sum and then do what I did for sum.world)
comparesums <- allstates.long %>% group_by(day) %>% summarize_at("count", sum,na.rm=T)
#comparesums
# seems coding was by US then by state at about 3/08 here is a command to
# produce total US counts for all days in the data set starting with 1/22/20 but
# it has lots of 0s
#data.frame(count=apply(allstates[,5:(ncol(allstates)-cadded)],2,sum,na.rm=T))
```



It is helpful to place a horizontal line in the cumulative US plot that corresponds to the 
US fraction of the world population. If covid-19 cases were randomly distributed throughout the world, we would expect a country to have a fraction of cases consistent with the fraction of that country's population relative to the world population.  The US is not doing well as our total number of cases are far above the red line that corresponds to the proportion of cases we would expect given our fraction of the global population. Of course, we don't know how well other countries are reporting their positive cases, how many tests they have administered, and other relevant information, but this descriptive plot suggests the rate of positive cases may be greater than expected so more investigation is needed.


```{r}
# assume 7.8 billion people in the world, the US has about 330 million
uspopfrac <- 330/7800
cv10frac <- uspopfrac*sum.world[nrow(sum.world),1]

# had to add +1 to max date in scale_x_date
p2 <- ggplot(allstates.long, aes(x=day,weight=count))  + geom_bar(na.rm=T,fill="steel blue") + ggtitle("US Counts") + scale_x_date(limits=c(sum.world$day[1], sum.world$day[nrow(sum.world)]+1))  + theme_minimal() + geom_hline(yintercept=cv10frac,col="red") + scale_y_continuous(labels=comma,limits=c(0,max(sum.world$count)))
# can print plot p2 by itself by uncommenting next line
#p2

#produce a single plot that has the world cumulative count at top and the US
#cumulative count at the botton, along with a red line indicating the count that
#corresponds to the fraction of the world population that represents the US if
#US counts are above the red line it means the counts of covid=19 are greater
#than what would be expected by a random process of covid-19 just appearing
#randomly based merely on population size alone

# the plot multiplot is defined in chapter preliminaries
multiplot(p1,p2,cols=1)
```


## US State-Level Plots

I decided to plot the percentage relative to population (i.e., counts/population * 100).  The numbers are small. I've seen people report this as cases per 100,000, but I decided to stick with cases per 100 to maintain the percentage interpretation.  This is just a scale issue and doesn't affect the plots or the analyses.   The labels in the plots, like "7.5 WA," mean Washington state with a population of 7.5 million. 

```{r fig.cap="Per Cap Positive Cases by State"}
# most recent date in file
max.day.numeric <- max(allstates.long$day.numeric)
max.day <- max(allstates.long$day)

# extend plot region on the right to allow room for state labels
extend.days <- 4

allstates.long%>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) +  theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm"))
```

Let's take the same plot but rescale the vertical axis on the log (base 10) scale.  An exponential process becomes linear when taking logs so we would expect to see a pattern that closely resembles straight lines if the count of confirmed cases follows an exponential form.  Each state can have a different growth rate, which will show up as different slopes, and different starting points, which will show up as different intercepts. 

A straight line seems to approximate the pattern but there  is a suggestion that some curvature remains, which implies these may not follow an exponential pattern. West Virginia (WV) didn't show its first case until 3/17/20 so that is why the green curve (lowest) looks different than the rest.

```{r fig.cap="Per Cap Positive Cases by State (log10 scale)"}
subset(allstates.long, day.numeric!=0) %>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) + 
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + scale_y_continuous(trans='log10')

```

To help see the state by state structure in these curves (e.g., whether or not they are linear in the log scale), I partitioned the 50 states by population (sample sizes in each panel, yielding 12 or 13 states per panel). The states with larger populations (two lower panels) show relatively linear patterns.  The states with smaller population sizes (the two upper panels) show patterns suggesting the per capita rate may still be increasing, whereas states with larger populations may be leveling off.   We would need statistical analyses to verify the observations in these descriptive plots.

```{r fig.cap="Per Cap Positive Cases by State (log10 scale); Panels are based on Population"}
subset(allstates.long, day.numeric!=0) %>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) + 
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + scale_y_continuous(trans='log10') + facet_wrap(~cut2(population, g=4,m=8))

```

I want to make a further improvement in this plot where I set 0 to missing value (NA) so that the curves begin where there are nonzero points. This modification to the plot makes it easier to detect linearity as we don't have the artificial jump in the curve from 0. But, while it makes the plots a little easier to understand, it drops the important information about _liftoff_, the point at which the count moves from zero to nonzero.  This is something that could be modeled with a parameter in the structural model that I will develop in later chapters and so, in principle, one could examine which factors affect the liftoff.  Another issue is that log(0) is undefined so to avoid 0s you'll see some websites and research papers showing such graphs requiring a minimum number of cases (e.g., the day at which the state reached 10 cases) before they start plotting the curve for that state. In the spirit of transparency one should report all these graphing decisions and ideally provide the code that produced the figures provided in a paper. Actually, complete reporting would include not only the code that produced the figure but also all code from the code that downloaded the data to the code that put the data in the format needed to produce the figure or table.

```{r  fig.cap="Per Cap Positive Cases by State (log10 scale); Panels are based on Population; Os dropped"}
subset(allstates.long, day.numeric!=0) %>%mutate(label = if_else(day == max(day), as.character(prettyval), NA_character_)) %>%
  mutate(count = na_if(count, "0")) %>%
ggplot( aes(x=day, y=count/population*100,group=prettyval, color=prettyval)) + geom_line() +
 geom_label_repel(aes(label= label), nudge_x=2, na.rm=T,segment.color = 'grey50', label.size=.01, size=2.5, show.legend=F) +
coord_cartesian(clip = 'off') + scale_x_date(limits=as.Date(c("2020-03-09", as.character(max.day+extend.days)))) + 
    theme(legend.position="none", plot.margin = margin(0.1, 1, 0.1, 0.1, "cm")) + scale_y_continuous(trans='log10') + facet_wrap(~cut2(population, g=4,m=8))

```

This pattern is  remarkable. The states appear to have similar slopes on this log plot. They vary in intercept, but that reflects when the state started reporting positive cases.  It seems states are on a similar growth trajectory, but some states are further along (higher intercepts) than other states.  That the log transformation of the Y axis converted these curves into approximate lines is consistent with the underlying pattern of these data following an exponential model. But we could argue that the slight curvature away from a straight line goes against an exponential growth process.  I'll say more about this in the next chapter where I'll cover the exponential more directly, both in its log linear form as in these graphs as well as through nonlinear regression models.

### Animated Map of US
\index{animation}

Here is the animation of the US.  This map needs work as Alaska and Hawaii outlines are not printed (their data points appear on the left side roughly where the states would be on the map).  I'll need to do some coding to create an inset for Alaska and Hawaii.

```{r cache=cache,fig.cap="US Count of Positive Cases", eval=knitr::is_html_output(), echo=longform&knitr::is_html_output()}
mybreaks <- c(1, 20, 100, 500, 1000, 2000, 10000)
mylabels <- c("1-19", "20-99", "100-499", "500-999", "1,000-1,999", "2,000-9,999","10,000+")

ggplot(allstates.long) +  borders("usa", colour = "gray90", fill = "gray85") +
 theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=count, color=count),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

```{r cache=cache,fig.cap="US Count of Positive Cases", fig.show="animate", aniopts=c("autoplay", "controls", "loop"),echo=longform&knitr::is_latex_output(), eval=knitr::is_latex_output()}
mybreaks <- c(1, 20, 100, 500, 1000, 2000, 10000)
mylabels <- c("1-19", "20-99", "100-499", "500-999", "1,000-1,999", "2,000-9,999","10,000+")

ggplot(allstates.long) +  borders("usa", colour = "gray90", fill = "gray85") +
 theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=count, color=count),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

Because I have the state-level population sizes I can redo the animation using a per capita normalization out of 100.  I modified the breaks and labels manually but there may be a way to do that automatically with ggplot2 features.

```{r cache=cache, fig.cap="US Per Cap (100)", eval=knitr::is_html_output(), echo=longform&knitr::is_html_output()}
temp <- cut2(allstates.long$percap100, g=5, m=5)
mylabels <- levels(temp)
#onlycuts = T saves the numeric break points, drop first element which is 0
mybreaks <- cut2(allstates.long$percap100, g=5, m=5, onlycuts=T)[-1]

ggplot(allstates.long) +  borders("usa", colour = "gray90", fill = "gray85") +
 theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=percap100, color=percap100),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

```{r cache=cache, fig.cap="US Per Cap (100)", fig.show="animate", aniopts=c("autoplay", "controls", "loop"), echo=longform&knitr::is_latex_output(),  eval=knitr::is_latex_output()}
temp <- cut2(allstates.long$percap100, g=5, m=5)
mylabels <- levels(temp)
#onlycuts = T saves the numeric break points, drop first element which is 0
mybreaks <- cut2(allstates.long$percap100, g=5, m=5, onlycuts=T)[-1]

ggplot(allstates.long) +  borders("usa", colour = "gray90", fill = "gray85") +
 theme_map() + 
  geom_point(aes(x=Long, y=Lat, size=percap100, color=percap100),stroke=F, alpha=0.7) +
 scale_size_continuous(name="Cases", trans="log", range=c(1,7),  breaks=mybreaks, labels = mylabels) +
    scale_color_viridis_c(option="inferno",name="Cases", trans="log",breaks=mybreaks, labels = mylabels) +
    theme_void() + 
    guides( colour = guide_legend()) +
    theme(
      legend.position = "bottom",
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#ffffff", color = NA), 
      panel.background = element_rect(fill = "#ffffff", color = NA), 
      legend.background = element_rect(fill = "#ffffff", color = NA)) + 
  labs(title="Date: {frame_time}", caption = "Data Repository provided by Johns Hopkins CSSE") +
  transition_time(day) + ease_aes('linear')
```

I don't like these plots because they use latitude and longitude to place a point on the graph to represent the entire state. It may be better to do this plot so that the entire area of the state is colored in according to the positive count rather than representing count as a single point.  Here is one example using the count of positive tests per state.  
```{asis echo=longform}
For this plot I used the fiftystater R package, which not only allows the shape of the state to be filled in but also includes an inset with Alaska and Hawaii.
```

```{r echo=longform}
# libraries just for this plot
library(RColorBrewer)
library(fiftystater)

temp <- subset(allstates.long, day==max.day)
temp$state <- tolower(temp$state.abb)

# set breaks on counts
b <- c(500, 5000,10000, 30000,100000,300000)

# plot uses log scale on count to make the colors vary 
p <- ggplot(temp, aes(frame = day, map_id = state)) +
  geom_map(aes(fill = count, color=count), color = "black", map = fifty_states) +
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(
    legend.position = "bottom",
    panel.background = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 24),
    legend.text = element_text(size = 16),
    legend.title = element_text(size = 16)
  ) +
  guides(fill = guide_legend(title = "US State Counts"), color="colorbar") +
  ggtitle("US State Counts of Positive Tests") +
  scale_fill_gradient(low = "yellow", high = "red", trans="log", breaks=b,
     labels=format(b, scientific=F))
p
```

```{exercise echo=T}
Repeat the same map but with the per capita positive tests instead of the total count data.
```

```{exercise echo=T}
Animate the map of the per capita positive count using the same additional commands to ggplot2 as you did for the previous animations.  The entire state will move from yellow to red as the number increases.
```


### US county-level plots

We can move to a different level of resolution with county-level data. 
I used the log10 scale on the number of positive tests for each US county to achieve a  smoother transition across the color range. White represents both counts of 0 and counts of 1.  This plot uses count and not the per capita count.

```{asis echo=longform}
For this plot I switched to the urbnmapr package.  This package needs to be downloaded from github. The first line in the chunk below (commented out) shows the call to download.   This package uses [FIPS](https://en.wikipedia.org/wiki/FIPS_county_code) rather than longitude and latitude, which makes it slightly easier to work with maps, including shading the entire region and putting Alaska and Hawaii as insets.
```

```{r}
#devtools::install_github("UrbanInstitute/urbnmapr")
library(urbnmapr)

# get county level data
newmap.US <- datacov.US[,c(1:11,dim(datacov.US)[2])]
recent.col <- dim(newmap.US)[2]

# set up the map
counties <- get_urbn_map(map="counties",sf=T)
counties$FIPS <- as.numeric(counties$county_fips)
counties <- counties %>% left_join(newmap.US, by=c("FIPS"))

# which column contains the counts
current.total <- which(names(counties)==names(newmap.US)[recent.col])

# compute log10 of counts
counties$logcount <- log10(c(counties[,current.total])[[1]])
counties$logcount <- ifelse(is.finite(counties$logcount), counties$logcount, 0)

# here are the plot commands
counties %>% ggplot() +
  geom_sf(mapping = aes(fill = logcount),
          size = 0.25, color="white") +
 scale_fill_gradient(low="white", high="blue", 
                     limits=range(counties$logcount,finite=T), oob=squish) +
  coord_sf(datum = NA) + ggtitle(paste0(
    "County log10 totals (",names(newmap.US)[recent.col],
    "); white represents counts of 0 and 1"))
```  


```{exercise echo=T}
Download the county-level population data from the US census following the example in Chapter \@ref(readintro) (see help in the ?fips_codes function in the tidycensus package). Merge these data with the counties data.frame used above. Compute the percentage of the county population that tests positive. Redraw the US map using the population percentages. You may find it helpful to replace the scale_fill_gradient command with something like scale_fill_gradientn(labels = scales::percent) to handle percentages more naturally. [Note: the death time series data file downloaded from the Johns Hopkins git repository has county-level population sizes as a column, which could be merged with the confirmed cases time series file we are using in this chapter.]
```

```{exercise echo=T}
Animate the county-level plot using the previous US animations as a template.
```
 
The county-level plot suggests some hypotheses to test. For example, in May there has been an increase in the number of protests over state social distancing restrictions and business closures.  One could examine associations between the number of protests and properties of the states such as the political party affiliation of the governor.  But with respect to county-level data, it appears in skimming the map of the US that some states have quite a bit of variability across counties in the positive test counts, whereas other states have relatively little variability with most counties having relatively high or relatively low positive test counts.  One hypothesis to check is whether protests are more common in states with greater variability across counties (that is, some counties not experiencing as many cases as other counties in the state).
 
## Incidence Plots {#incidenceplots}

With time series data it is common to work with what are called first order differences. Rather than examine cumulative counts you look at day to day differences in the cumulative counts, or equivalently, the number of new cases each day.  This is what give rise to the "curves" when people talk about "flattening the curve". Think of this as daily counts and we merely compute histograms.  This section was adapted from [Tim Chruches Blog](https://timchurches.github.io/blog/posts/2020-02-18-analysing-covid-19-2019-ncov-outbreak-data-with-r-part-1/).

Here is the daily incidence rate using the same US data starting on 3/10/20. This plot does not include DC and the territories.   I also include a 7 day moving average (blue curve). This is the curve traced by computing the mean of the 7 previous days and using that value for the day's count, then tracing those points with a curve. In the month of May many news outlets began including such moving average summaries rather than the raw data. They make the data look less variable than they actually are, which is not a good thing when modeling data.  We want our models to take into account the variability in the data rather than mask the variability.
\index{moving average}


```{r incplot}
allstates.temp <- allstates[,c(1:4,52:(ncol(allstates)-cadded))]
allstates.temp <- allstates.temp[,-5]

# create new data frame with differences of total counts which equals daily totals
day.sums <- dplyr::select(allstates.temp, `3/10/20`:ncol(allstates.temp)) %>% colSums(na.rm=T)
day.sums <- data.frame(day = as.Date(names(day.sums),"%m/%d/%y"), count = day.sums)
# this is the line that computes differences
day.sums$daily <- c(NA,diff(day.sums$count))

# compute 7 day moving average
day.sums <- day.sums %>% mutate(movavg=movavg(daily, n=7, type="s"))

# make plot with nice labels 5th bar looks almost empty; the data show 61 cases
# that day
ggplot( day.sums, aes(x=day,y=daily)) +  geom_bar(stat="identity") +
    scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
    labs(y="Daily incremental incidence",
                                   title="Positive US Covid-19 Cases")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11)) +
      geom_line(aes(x=day, y=movavg), color="blue", size=1.5) 
```

\index{interactive plot}
We can add  interactive capabilities to the ggplot by using the ggigraph package.  
```{asis echo=longform}
The package needs to be downloaded through github; the path is commented out in the R code chunk.
```
The html version of this book allows popups when the mouse hovers over the bars. You will notice that the number of positive deaths has a weekly pattern emerging on Monday April 13, 2020. There is a trough that occurs weekly on Mondays through at least May 11.  I will monitor on subsequent weeks.  This is an important part of the variability these data exhibit, which is lost if we "smooth" the data by computing moving averages as shown in the blue curve of the previous graph.

```{r eval=knitr::is_html_output(), echo=longform&knitr::is_html_output()}
# devtools::install_github('davidgohel/ggiraph')
library(ggiraph)

# code to format a table to display on mouse hover
str_model <- paste0("<tr><td>date</td><td>%s</td></tr>", 
                    "<tr><td>day</td><td>%s</td></tr>", 
                   "<tr><td>count</td><td>%s</td></tr>")
day.sums$tooltip <- sprintf(str_model, day.sums$day, 
                            weekdays(day.sums$day),
                            day.sums$daily)
day.sums$tooltip <- paste0( "<table>", 
                            day.sums$tooltip, "</table>" )

p <- ggplot( day.sums, aes(x=day,y=daily, tooltip = tooltip,
        data_id = day)) +  geom_bar_interactive(stat="identity") +
    scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
    labs(y="Daily incremental incidence",
                                   title="Positive US Covid-19 Cases (mouse hover in html version")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))

girafe(ggobj=p)
```

The weekly pattern is interesting and could be explained, for example, by fewer tests conducted on weekends with a delay in the reporting of tests results.  We can check this by examining another data set that reports the total number of tests conducted on a day, which is the sum of the number of positive tests, the number of negative tests and the number of pending tests. Of course, the number of positive tests will be correlated with this total measure, and this definition of total introduces some additional daily correlations due to some double counting (i.e., a pending today may show up as a negative tomorrow once the results are known). Further, this highlights the difficult of interpreting such data, for many reasons, including the issue that  tests vary in the length of time needed to achieve a result, so if states vary in which tests they use or there are changes over time in which tests are used, we would see such patterns reflected in these counts.  Such changes may say more about the decisions made on which tests to use than on the changing properties of the distribution of positive and negative test results.

The bar plot of the total test results is presented in the next figure.  While the number of tests administered is increasing over the weeks, there is a slight pattern that fewer tests are reported on  Mondays relative to surrounding days, especially in recent weeks (e.g., April 27 and May 4).

```{r}
covid.tracking.date <- covid.tracking %>% subset(date>as.Date("2020-03-9", "%Y-%m-%d"))
covid.tracking.date <- covid.tracking.date %>% group_by(date) %>% dplyr::summarize(date.sum = sum(total, na.rm=T)) 
covid.tracking.date <- covid.tracking.date %>% mutate(sumdaily = c(date.sum[1], diff(date.sum)))

ggplot( covid.tracking.date, aes(x=date,y=sumdaily)) +  geom_bar(stat="identity") +
    scale_x_date(date_breaks="1 week", date_labels = "%d %b") +
    labs(y="Daily incremental incidence",
                                   title="Total US Tests (Pos, Neg and Pending)")  +
    theme(legend.position = "none",
          strip.text.y = element_text(size=11))
```

Overall, we see that the number of daily tests has been increasing over recent weeks. This adds an important caveat moving forward as we interpret positive tests results in the remainder of this book.  We may see an increase in positive tests not because virus contamination is increasing but because there are more tests being conducted.  Of course, if the number of positive cases decreases in spite of the number of tests increasing, this suggests the virus rate is decreasing. Such a conclusion assumes the characteristics of the tests remain the same. If over time, the quality of the tests  changes in terms of false positives and false negatives, that would add an additional layer of complexity in interpreting such testing data.

## Using PCA to check for outliers
\index{PCA}
\index{outliers}

Principal components analysis (PCA) has many uses.  In interesting use of PCA in time series data is that it can be used to detect outliers in a complicated data set. Let's take the 50 states and the time series from 3/09/20 to present, to create a 50 by day data matrix of positive counts.  Then compute a PCA of the correlation matrix between states. This correlation matrix is 50 x 50 and represents how similar each state's cumulative trajectory is to another state's cumulative trajectory for all possible pairs of states.  Plot the factor scores of the 50 states on the first two PCs.  The majority of the states will cluster together. The outlier states, those with very different trajectories, appear far away from the primary cluster suggesting they have a very different trajectory. The three candidate outliers are the 4 states with early covid-19 cases:  NY, WA, NJ and CA. Recall that the numbers in front of the state abbreviation correspond to the state population in millions.  This type of PCA is commonly done in biological modeling to check for outliers, sometimes not on the raw data like I did here, but on the set of parameters that are estimated for each unit. For example, if you run regressions for each state, gather the betas from those regressions as data (such as 8 betas per state if you had 8 state-level predictors),  compute a correlation matrix across states, then run a PCA on that correlation matrix to detect states with an outlier pattern across their betas relative to other states. 

```{r fig.cap="PCA to identify states with candidate outliers"}
temporal.data <- allstates[,53:(ncol(allstates)-cadded)]
rownames(temporal.data) <- allstates[,"prettyval"]$prettyval

pca.out <- prcomp(temporal.data, center=T, scale=T)

ggplot(data.frame(PC1=pca.out$x[,1], PC2=pca.out$x[,2], labels=rownames(pca.out$x)), aes(x=PC1, y=PC2, label=labels)) + geom_text(size=4)

```

## Summary

In this chapter I focused on country, state and county-level analyses. Of course, finer resolution could be at by zip code, or any other sensible way of partitioning the map.  A different type of partition could be in how hospitals are structured in an organized system across the US. There are about 340 "hospital referral regions." These are regions where there is a primary hospital that can handle specialized cardiovascular procedures and several other hospitals, perhaps part of different systems, that refer patients needing these specialized services to the primary hospital.  Or even more fine-grained, there are "hospital service areas", which make up zip codes that are serviced by a particular hospital. Here are such [maps for covid](https://www.dartmouthatlas.org/covid-19/).  When one starts adding predictors or outcomes of these trajectories, then the level of resolution becomes critical.  If one wanted to examine the contributors to health disparities around covid-19 and how they impact the properties of the trajectories, then one should consider matching the level of analysis of the trajectory (state, county, zip code, hospital service area, population density, etc.) with corresponding predictors at similar levels such as predictors relevant to government expenditures, measures of income inequality, indices of urbanization, unemployment rates, etc. (that is, each of those predictors would be evaluated at the similar level as the trajectory data).
